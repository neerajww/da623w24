<p><strong>Suggested Readings:</strong></p>
<ul>
  <li><a href="https://ruder.io/optimizing-gradient-descent/">Optimization update rules by Sebastian Ruder</a></li>
  <li><a href="https://proceedings.mlr.press/v28/sutskever13.html">SGD with Momentum, I Sutskever et al. ICML 2013</a></li>
  <li><a href="https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Ada Grad, Duchi et al. JMLR, 2011</a></li>
  <li><a href="https://arxiv.org/abs/1412.6980">Adam, Kingma et al. ICLR 2015</a></li>
  <li><a href="https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10">Some insgihts about lr in dl</a></li>
  <li><a href="https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">Random search for hyper-parameter selection, Bergstra, JMLR 2012</a></li>
</ul>

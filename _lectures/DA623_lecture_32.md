---
type: lecture
date: 2023-04-28
title: Lecture-32

# optional
# please use /static_files/notes directory to store notes
# thumbnail: /static_files/path/to/image.jpg

# optional
tldr: "Why reduce dimensions? Getting into PCA and LDA"
# optional
# set it to true if you dont want this lecture to appear in the updates section
hide_from_announcments: false

# optional
links: 
    #- url: /static_files/presentations/lec.zip
    #  name: notes
    #- url: /static_files/presentations/code.zip
    #  name: codes
    - url: empty
      name: board
    #- url: /static_files/presentations/lec.zip
    #  name: other
---

**Reading:**
- [Chap. 12: PCA](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) by C. Bishop.
- [Section 4.1.4 Fisher's linear discriminant](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) by C. Bishop.

**Code:**
- [PCA](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)
- [LDA and PCA](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html)
